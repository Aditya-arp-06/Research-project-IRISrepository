import pandas as pd
import numpy as np
import math
from scipy.stats import chisquare, chi2
import os
import sys
import tkinter as tk
from tkinter import filedialog

# ==========================================
# CONFIGURATION
# ==========================================
NUM_BINS = 50  # Matches your Critical Limit ~66.34
SIGNIFICANCE_LEVEL = 0.05

def select_file():
    root = tk.Tk()
    root.withdraw()
    print("[-] Waiting for user to select CSV file...")
    file_path = filedialog.askopenfilename(
        title="Select AADS Results CSV",
        filetypes=[("CSV Files", "*.csv")]
    )
    if not file_path:
        print("[!] No file selected. Exiting.")
        sys.exit()
    return file_path

def run_elite_analysis(csv_path):
    print(f"[*] Loading dataset: {os.path.basename(csv_path)}...")
    try:
        df = pd.read_csv(csv_path)
        # Attempt to auto-detect the column. If 'Constant_C' exists, use it.
        # Otherwise, take the first column.
        if 'Constant_C' in df.columns:
            data = df['Constant_C'].values
        else:
            data = df.iloc[:, 0].values
            
        n = len(data)
    except Exception as e:
        print(f"[!] Error reading CSV: {e}")
        return

    print(f"[*] Analyzing {n:,} constants...")
    print("-" * 60)

    # ==========================================
    # 1. REAL SHANNON ENTROPY (The Rigorous Way)
    # ==========================================
    # We calculate H based on actual probability distribution P(x)
    # H = -SUM(p(x) * log2(p(x)))
    
    value_counts = pd.Series(data).value_counts()
    probs = value_counts / n
    observed_entropy = -np.sum(probs * np.log2(probs))
    
    theoretical_max = math.log2(n)
    
    print(f"SHANNON ENTROPY ANALYSIS")
    print(f"------------------------")
    print(f"Sample Size (N)       : {n:,}")
    print(f"Unique Values Found   : {len(value_counts):,}")
    print(f"Observed Entropy      : {observed_entropy:.4f} bits")
    print(f"Theoretical Maximum   : {theoretical_max:.4f} bits")
    
    # Calculate "Information Loss" (collisions)
    info_loss = 100 * (1 - (observed_entropy / theoretical_max))
    print(f"Information Loss      : {info_loss:.6f}%")
    
    if info_loss < 0.000001:
        print(f"Interpretation        : PERFECT RANDOMNESS (No collisions detected)")
    else:
        print(f"Interpretation        : Minor Bias Detected")
        
    print("-" * 60)

    # ==========================================
    # 2. CHI-SQUARE UNIFORMITY TEST
    # ==========================================
    print(f"CHI-SQUARE UNIFORMITY TEST")
    print(f"--------------------------")
    
    # Binning logic
    observed_counts, bin_edges = np.histogram(data, bins=NUM_BINS)
    expected_count = n / NUM_BINS
    expected_counts = np.full(NUM_BINS, expected_count)
    
    # Chi-Square Calculation
    chi2_stat = np.sum((observed_counts - expected_counts)**2 / expected_counts)
    dof = NUM_BINS - 1
    critical_val = chi2.ppf(1 - SIGNIFICANCE_LEVEL, dof)
    
    print(f"Number of Bins        : {NUM_BINS}")
    print(f"Degrees of Freedom    : {dof}")
    print(f"Test Statistic (X2)   : {chi2_stat:.2f}")
    print(f"Critical Value (0.05) : {critical_val:.2f}")
    
    print(f"\nRESULT:")
    if chi2_stat < critical_val:
        print(f"[PASSED] The distribution is statistically Uniform.")
    else:
        print(f"[NOTE] Null Hypothesis Rejected (Significant deviation).")

    print("-" * 60)

if __name__ == "__main__":
    file = select_file()
    run_elite_analysis(file)
