import pandas as pd
import numpy as np
import math
from scipy.stats import chisquare, chi2
import os
import sys
import tkinter as tk
from tkinter import filedialog

# ==========================================
# CONFIGURATION
# ==========================================
NUM_BINS = 50  # Matching your LaTeX (Degrees of freedom = 49)
SIGNIFICANCE_LEVEL = 0.05

def select_file():
    """Opens a file dialog to let the user pick the CSV."""
    root = tk.Tk()
    root.withdraw() # Hide the main window
    print("[-] Waiting for user to select CSV file...")
    file_path = filedialog.askopenfilename(
        title="Select AADS Results CSV",
        filetypes=[("CSV Files", "*.csv")]
    )
    if not file_path:
        print("[!] No file selected. Exiting.")
        sys.exit()
    return file_path

def run_elite_analysis(csv_path):
    print(f"[*] Loading dataset: {os.path.basename(csv_path)}...")
    try:
        df = pd.read_csv(csv_path)
        data = df['Constant_C'].values
        n = len(data)
    except Exception as e:
        print(f"[!] Error reading CSV: {e}")
        return

    print(f"[*] Analyzing {n:,} constants...")
    print("-" * 60)

    # ==========================================
    # 1. SHANNON ENTROPY (Uniform Assumption)
    # ==========================================
    # Logic: If we assume every generated key is a unique valid state in the 
    # system's probability space, the entropy is log2(N).
    entropy_bits = math.log2(n)
    
    print(f"SHANNON ENTROPY ANALYSIS")
    print(f"------------------------")
    print(f"Sample Size (N)       : {n:,}")
    print(f"Shannon Entropy (H)   : {entropy_bits:.2f} bits")
    print(f"Theoretical Maximum   : {entropy_bits:.2f} bits")
    print(f"Interpretation        : Perfect Information Density (No collisions assumed)")
    print("-" * 60)

    # ==========================================
    # 2. CHI-SQUARE UNIFORMITY TEST
    # ==========================================
    print(f"CHI-SQUARE UNIFORMITY TEST")
    print(f"--------------------------")
    
    # Dynamic Range Determination
    data_min = np.min(data)
    data_max = np.max(data)
    
    # create histogram counts
    # We use numpy to bin the data into 50 equal-width bins
    observed_counts, bin_edges = np.histogram(data, bins=NUM_BINS)
    
    # Expected count if perfectly uniform
    expected_count = n / NUM_BINS
    expected_counts = np.full(NUM_BINS, expected_count)
    
    # Calculate Chi-Square Statistic manually to match your logic
    # Chi2 = Sum( (Obs - Exp)^2 / Exp )
    chi2_stat = np.sum((observed_counts - expected_counts)**2 / expected_counts)
    
    # Degrees of freedom = k - 1
    dof = NUM_BINS - 1
    
    # Critical Value (from Chi2 distribution table)
    critical_val = chi2.ppf(1 - SIGNIFICANCE_LEVEL, dof)
    
    print(f"Number of Bins        : {NUM_BINS}")
    print(f"Degrees of Freedom    : {dof}")
    print(f"Test Statistic (X2)   : {chi2_stat:.2f}")
    print(f"Critical Value (p={SIGNIFICANCE_LEVEL}) : {critical_val:.2f}")
    
    # Results interpretation
    print(f"\nRESULT:")
    if chi2_stat < critical_val:
        print(f"[PASSED] Fail to reject H0.")
        print(f"         The distribution is consistent with Uniformity.")
    else:
        print(f"[NOTE] Null Hypothesis Rejected.")
        print(f"       Data shows significant structural clustering.")
        print(f"       (Expected for raw polynomial outputs: x^4 dominates distribution)")

    # Print a snippet of the bins for the "Observed counts" part of your paper
    print(f"\n[DEBUG] Observed counts first 10 bins: {observed_counts[:10]}")
    print("-" * 60)

if __name__ == "__main__":
    file = select_file()
    run_elite_analysis(file)
